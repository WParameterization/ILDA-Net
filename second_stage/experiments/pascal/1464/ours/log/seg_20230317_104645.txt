Using TensorFlow backend.
[2023-03-17 10:46:51,433][    INFO] {'criterion': {'kwargs': {'use_weight': False}, 'type': 'CELoss'},
 'dataset': {'batch_size': 16,
             'ignore_label': 255,
             'mean': [123.675, 116.28, 103.53],
             'n_sup': 195,
             'noise_std': 0.1,
             'std': [58.395, 57.12, 57.375],
             'train': {'GaussianBlur': False,
                       'crop': {'size': [256, 256], 'type': 'rand'},
                       'data_list': '../../../../data/splits/pascal/1464/labeled.txt',
                       'data_root': '../../../../data/VOC2012',
                       'flip': True,
                       'rand_resize': [0.5, 2.0]},
             'type': 'pascal_semi',
             'val': {'crop': {'size': [256, 256], 'type': 'center'},
                     'data_list': '../../../../data/splits/pascal/val.txt',
                     'data_root': '../../../../data/VOC2012'},
             'workers': 1},
 'exp_path': '',
 'net': {'bo': {'type': 'u2pl.models.model_helper.Bottleneck'},
         'decoder': {'type': 'u2pl.models.model_helper.Decoder'},
         'ema_decay': 0.99,
         'encoder': {'type': 'u2pl.models.model_helper.Encoder'},
         'fc_inch': '121,',
         'filters': '32,',
         'in_channels': '3,',
         'n_class': '9,',
         'num_classes': 9,
         'sync_bn': True},
 'save_path': 'checkpoints101_rgb',
 'saver': {'pretrain': '', 'snapshot_dir': 'checkpoints101_rgb'},
 'trainer': {'contrastive': {'current_class_negative_threshold': 1,
                             'current_class_threshold': 0.3,
                             'high_rank': 20,
                             'low_entropy_threshold': 20,
                             'low_rank': 3,
                             'negative_high_entropy': True,
                             'num_negatives': 101,
                             'num_queries': 256,
                             'temperature': 0.5,
                             'unsupervised_entropy_ignore': 80},
             'epochs': 80,
             'eval_on': True,
             'lr_scheduler': {'kwargs': {'power': 0.9}, 'mode': 'poly'},
             'optimizer': {'kwargs': {'lr': 0.001,
                                      'momentum': 0.9,
                                      'weight_decay': 0.0001},
                           'type': 'SGD'},
             'unsupervised': {'TTA': False,
                              'apply_aug': 'cutmix',
                              'drop_percent': 80}}}
set random seed to 2
[2023-03-17 10:46:53,527][    INFO] # samples: 693
[2023-03-17 10:46:53,529][    INFO] # samples: 207
[2023-03-17 10:46:53,529][    INFO] Get loader Done...
[2023-03-17 10:46:54,863][    INFO] The kwargs for lr scheduler: 0.9
[2023-03-17 10:46:59,945][    INFO] [195][none] Iter [0/9760]	Data 0.00 (0.00)	Time 4.94 (4.94)	Sup 2.911 (2.911)	Uns 0.000 (0.000)	Con 0.000 (0.000)	LR 0.00100
Traceback (most recent call last):
  File "../../../../train_semi_vit.py", line 512, in <module>
    main()
  File "../../../../train_semi_vit.py", line 174, in main
    train( model, model_teacher, optimizer,lr_scheduler,sup_loss_fn, train_loader_sup,epoch,tb_logger,logger,memobank,queue_ptrlis,queue_size,)
  File "../../../../train_semi_vit.py", line 252, in train
    _ = model_teacher(image_l)#teacher模型也要跟着训练一边但是没有loss函数控制
  File "/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/root/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 619, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/root/autodl-tmp/U2PL/u2pl/models/vit_transformer_yindaolvbo_panbieqi.py", line 474, in forward
    output=self.end(x_1)
  File "/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/root/autodl-tmp/U2PL/u2pl/models/vit_transformer_yindaolvbo_panbieqi.py", line 560, in forward
    res["rep"] = self.representation(x)
  File "/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 524, in forward
    return F.batch_norm(
  File "/root/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py", line 2056, in batch_norm
    return torch.batch_norm(
RuntimeError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 23.70 GiB total capacity; 19.78 GiB already allocated; 470.56 MiB free; 21.85 GiB reserved in total by PyTorch)
Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/root/miniconda3/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/root/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py", line 260, in <module>
    main()
  File "/root/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py", line 255, in main
    raise subprocess.CalledProcessError(returncode=process.returncode,
subprocess.CalledProcessError: Command '['/root/miniconda3/bin/python', '-u', '../../../../train_semi_vit.py', '--local_rank=0', '--config=config.yaml', '--seed', '2', '--port', '1255']' returned non-zero exit status 1.
